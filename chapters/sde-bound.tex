\begin{savequote}[75mm]
    The theory of probability is the only mathematical tool available to help map the unknown and the uncontrollable. It is fortunate that this tool, while tricky, is extraordinarily powerful and convenient.
\qauthor{Benoit Mandelbrot}
\end{savequote}

\chapter{Error Bound for Linear SDE}  \label{chapter:error-bound-for-linear-sdes}
    In addition to deterministic differential equations, we also consider a class of linear stochastic differential equations (SDEs) in this thesis by introducing an additional stochastic term to the forcing function. 
    For any given network, we aim to bound the error at any point with arbitrary high probability using the network's residual on the deterministic part of the equation.
\section{Formulation}
    In this chapter, we deal with linear first-order SDEs of the form
    \begin{equation}\label{eq:linear-sde}
        \d V_t = \big(\lambda V_t + f(t)\big)\d t + g(t) \d W_t \quad \text{s.t.} \quad V_0 = v_0 
    \end{equation}
    where $W_t$ is a Wiener process. 
    We assume that $f(t)$ and $g(t)$ are deterministic functions and $\lambda \in \mathbb{R}$ is a constant. 
    We denote the solution with capital $V_t$ because it is a continuous stochastic process whose realization depends on the realization of $W_t$.

    Unlike with deterministic differential equations, we do not evaluate the residual $r(t)$ of a network solution $u(t)$ on the SDE because the stochastic term $g(t) \d W(t)$ is not deterministic.
    Instead, we evaluated the residual on the following deterministic differential equation
    \begin{equation}
        \E{\d V_t} = \E{\big(\lambda V_t + f(t)\big)\d t + g(t) \d W_t}
    \end{equation}
    Assuming we can change the order of expectation and differentiation, we have
    \begin{equation}
        \d v(t) = (\lambda v(t) + f(t))\d t
    \end{equation}
    where $v(t) = \E{V(t)}$. 
    Since this is a deterministic differential equation, we can evaluate the residual of neural network $u(t)$ on this equation. 
    Namely,
    \begin{equation}\label{eq:linear-sde-network}
        \d u(t) = \big( \lambda u(t) + f(t)\big) \d t + r(t)\d t.
    \end{equation}
    We aim to formulate a bound for the absolute error $|\Eta_t| = |u(t) - V_t|$ using only $\lambda$, $f(\cdot)$, and $g(\cdot)$. 
    Since the error $\Eta_t$ is a stochastic process, we shall formulate an upper tail bound for the distribution of $\Eta_t$. 
    Namely, we aim to formulate a bound $|\Eta| \leq \Bound_\epsilon$ that holds true with arbitrarily high probability $1 - \epsilon$,
    \begin{equation}\label{eq:upper-tail-bound-goal}
        \P{\, \big|\Eta_T\big| \leq \Bound_\epsilon(t)} \geq 1 - \epsilon \quad \forall t \in I.
    \end{equation}
\section{Upper Tail Bound of Absolute Error}
    To derive a tail bound Eq. \eqref{eq:upper-tail-bound-goal}, we take the difference between Eq. \eqref{eq:linear-sde-network} and Eq. \eqref{eq:linear-sde},
    \begin{equation}
        \d \Eta_t  = \big(\lambda \Eta_t + r(t)\big)\d t - g(t) \d W_t
    \end{equation}
    It is well known the the solution of this equation is
    \begin{equation}
        \Eta_t = \I_{\lambda} r(t) - e^{\lambda t} \int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau
    \end{equation}
    Since $\lambda \in \mathbb{R}$, by the triangle inequality and property \eqref{eq:inverse-operator-inequality} of operator $\I_{\lambda}$, there is
    \begin{align}\label{eq:eta-bound-unevaluated}
        \big|\Eta_t\big| &\leq \big|\I_{\lambda} r(t)\big| + \left|e^{\lambda t} \int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau\right|\\
        &\leq \I_{\Re{\lambda}} |r(t)| + \left|e^{\lambda t} \int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau\right|\\
        &= \I_{\lambda} |r(t)| + e^{\lambda t} \left|\int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau\right|\label{eq:eta-bound-unevaluated}
    \end{align}
    where the first term $\I_{\lambda} |r|$ can be bounded using either Alg. \ref{alg:single-linear-ode-constant-coeff-loose} or Alg. \ref{alg:single-linear-ode-constant-coeff-tight}.
    For the second term, notice that for almost every realization of $\d W_t$, the integral $\displaystyle \int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau$ is a Riemann-Stieltjes integral and can be evaluted by parts.
    Namely, 
    \begin{align}
        e^{\lambda t} \left|\int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau\right| &= e^{\lambda t} \left|  e^{-\lambda t} g(t) W_t - 1 \cdot g(0) W_0 - \int_{0}^{t} W_\tau \d \big(e^{-\lambda\tau}g(\tau)\big)\right| \\
        &= e^{\lambda t} \left|  e^{-\lambda t} g(t) W_t - \int_{0}^{t} e^{-\lambda\tau}\big(g'(\tau) - \lambda g(\tau)\big) W_\tau \d \tau\right| \\
        &= \left|  g(t) W_t - e^{\lambda t} \int_{0}^{t} e^{-\lambda\tau}\big(g'(\tau) - \lambda g(\tau)\big) W_\tau \d \tau\right| \\
        &\leq \left| g(t)W_t \right| + \left|e^{\lambda t} \int_{0}^{t} e^{-\lambda\tau}\big(g'(\tau) - \lambda g(\tau)\big) W_\tau \d \tau\right|
    \end{align}
    Let $p(\tau) = e^{-\lambda\tau}\big(g'(\tau) - \lambda g(\tau)\big)$.
    By the Cauchy-Schwarz inequality, there is
    \begin{align}
        e^{\lambda t} \left|\int_0^t e^{-\lambda \tau} g(\tau) \d W_\tau\right| &\leq \left| g(t)W_t \right| + \left|e^{\lambda t} \int_{0}^{t} p(\tau) W_\tau \d \tau\right| \\
        &\leq \left| g(t)W_t \right| + e^{\lambda t} \sqrt{\int_{0}^{t} p^2(\tau) \d \tau} \sqrt{\int_{0}^{t} W_\tau^2 \d \tau}\label{eq:bound-of-stochastic-part-unsolved}
    \end{align}
    Since $W_t$ is a Wiener process, there is $W_t \sim \mathcal{N}(0, t)$.
    It is well known that the tail bound of the normal distribution $\mathcal{N}(0, t)$ is given by
    \begin{equation}\label{eq:tail-bound-epsilon-1}
        \P{|W_t| \geq \sqrt{2t\ln(2/\epsilon_1)}} \leq \epsilon_1 \quad \forall \epsilon_1 \in (0, 1)
    \end{equation}
    The distribution of $\displaystyle \int_{0}^{t} W_\tau^2 \d \tau$ is more complicated and was first studied by \citeauthor{cameron1944transformations} \cite{cameron1944transformations}.
    We shall use the fact that $\displaystyle \E{\int_{0}^{t}W_\tau^2\d \tau} = \frac{t^2}{2}$ and $\displaystyle \mathrm{Var}\left[\int_{0}^{t}W_\tau^2\d \tau\right] = \frac{t^4}{3}$, and apply Chebyshev-Cantelli inequality to obtain the following tail bound
    \begin{equation}
        \P{\int_0^t W_\tau^2 \d \tau \geq \left(\frac{1}{2}+\rho\right)t^2} \leq \frac{1}{1+3\rho^2} \quad \forall \rho \geq 0.
    \end{equation}
    Let $\epsilon_2 = \frac{1}{1 + 3\rho^2}$, the above tail inequality can be rewritten as
    \begin{equation}\label{eq:tail-bound-epsilon-2}
        \P{\int_{0}^{t} W_\tau^2 \d \tau \geq \left(\frac{1}{2}+\sqrt{\frac{1}{3\epsilon_2} - \frac{1}{3}}\right)t^2} \leq \epsilon_2 \quad \forall \rho \geq 0.
    \end{equation}
    Applying union bound over Eq. \eqref{eq:tail-bound-epsilon-1} and \eqref{eq:tail-bound-epsilon-2} and substituting into Eq. \eqref{eq:bound-of-stochastic-part-unsolved} and \eqref{eq:eta-bound-unevaluated}, there is 
    \begin{equation}
        \P{|\Eta_t| \geq \Bound'_{\epsilon_1, \epsilon_2}(t)} \leq \epsilon_1 + \epsilon_2,
    \end{equation}
    where
    \begin{gather}\label{eq:bound-two-epsilon}
        \Bound'_{\epsilon_1, \epsilon_2} (t) = \I_{\lambda} |r| + |g(t)|\sqrt{2t\ln(2/\epsilon_1)}+ te^{\lambda t} \sqrt{\frac{1}{2}+\sqrt{\frac{1}{3\epsilon_2} - \frac{1}{3}}} \sqrt{\int_{0}^{t} p^2(\tau) \d \tau}.
    \end{gather}
    In order to find a bound $\Bound_\epsilon$ such that 
    \begin{equation}\label{eq:sde-bound-probability}
        \P{|\Eta_t| \leq 1 - \Bound_{\epsilon}(t)} \geq \epsilon,
    \end{equation}
    one can choose 
    \begin{equation}\label{eq:sde-bound-construction}
        \Bound_{\epsilon}(t) = \Bound'_{\epsilon_1, \epsilon_2}(t),
    \end{equation}
    where $\epsilon_1 + \epsilon_2 = \epsilon$ and $\epsilon_1, \epsilon_2 > 0$.
    A default choice is to set $\epsilon_1 = \epsilon_2 = \dfrac{\epsilon}{2}$.
    A tighter (and more time consuming) bound can be achieved by setting
    \begin{equation}
        \epsilon_1^*, \epsilon_2^* = \argmin_{\substack{\epsilon_1, \epsilon_2 > 0 \\ \epsilon_1 + \epsilon_2 = 1}} \Bound'_{\epsilon_1, \epsilon_2}(t)
    \end{equation}
    where the $\argmin$ is evaluated over a dense grid.

    Similar to Alg. \ref{alg:single-linear-ode-constant-coeff-loose} and \ref{alg:single-linear-ode-constant-coeff-tight} in Chapter \ref{chapter:error-bound-for-linear-odes}, we construct Alg. \ref{alg:sde} to bound the error between a network solution and any realization of an true SDE solution.
    
    \makeatletter
    \setlength{\@fptop}{0pt}
    \begin{algorithm}[!htbp]
        \caption{Error Bound Estimation for SDE \eqref{eq:linear-sde}}\label{alg:sde}
        \textbf{Input:} Coefficient $\lambda$, residual information $r(t)$ on the deterministic equation \eqref{eq:linear-sde-network}, noise strength $g(t)$, maximum probability $\epsilon$ for error bound to fail, and a sequence of time points $\left\{t_\ell\right\}_{\ell=1}^{L}$ where error bound is to be evaluated\\
        \textbf{Output:} Error bound values $\left\{\Bound_\epsilon(t_\ell)\right\}_{\ell=1}^{L}$ at given time points
        \begin{algorithmic}
            \Ensure $\P{\left|\Eta_{t_\ell}\right| \leq \Bound_\epsilon(t_\ell)} \geq 1 - \epsilon$ for all $\ell$
            \State $\{\Bound_{\text{deter}}(t_\ell)\}_{\ell=1}^{L} \gets $ {\small Call Alg. }\ref{alg:single-linear-ode-constant-coeff-loose} {\small or }\ref{alg:single-linear-ode-constant-coeff-tight} {\small with } $\lambda${\small, }$r(\cdot)${\small, and }$\left\{t_\ell\right\}_{\ell=1}^{L}$
            \State $p(\tau) \gets e^{-\lambda\tau}\big(g'(\tau) - \lambda g(\tau)\big)$
            \State $\left\{\|p\| (t_\ell)\right\}_{\ell=0}^L \gets \{L^2$ {\small norm of} $p(\tau)$ {\small over} $[0, t_\ell]$ {\small computed numerically} $\}_{\ell=1}^{L}$
            \State $\epsilon_1, \epsilon_2 \gets \frac{\epsilon}{2}, \frac{\epsilon}{2}$ \Comment{{\color{gray} Default choice, can be changed for tighter bound}}
            \State $\{\Bound_{\text{stoch}, 1}(t_\ell)\}_{\ell=1}^{L} \gets \big\{|g(t_\ell) \sqrt{2t_\ell \ln(2/\epsilon_1)}\big\}_{\ell=1}^{L}$
            \State $\{\Bound_{\text{stoch}, 2}(t_\ell)\}_{\ell=1}^{L} \gets \Big\{t_\ell e^{\lambda t_\ell} \|p\|(t_\ell) \sqrt{1/2+ \sqrt{1/(3\epsilon_2) - 1/3}} \Big\}_{\ell=1}^{L}$
            \State $\{\Bound_{\epsilon}(t_\ell)\}_{\ell=1}^L \gets \big\{ \Bound_{\text{deter}}(t_\ell) + \Bound_{\text{stoch}, 1}(t_\ell) + \Bound_{\text{stoch}, 2}(t_\ell) \big\}_{\ell=1}^{L}$
            \State \textbf{return} $\left\{\Bound_\epsilon(t_\ell)\right\}_{\ell=1}^{L}$
        \end{algorithmic}
        % \vspace{128in}
    \end{algorithm}
    \makeatother